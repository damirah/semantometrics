Why the impact factor of journals should not be used for evaluating research
Evaluating scientific quality is a notoriously difficult problem which has no standard solution. Ideally, published scientific results should be scrutinised by true experts in the field and given scores for quality and quantity according to established rules. In practice,however,  what  is  called  peer  review  is  usually performed by committees with general competence rather than with the specialist's insight that is needed to assess primary research data. Committees tend, there≠fore,  to  resort  to  secondary  criteria  like  crude publication counts, journal prestige, the reputation of authors and institutions, and estimated importance and relevance of the research field,1making peer review as much of a lottery as of a rational process.23On this background, it is hardly surprising that alternative methods for evaluating research are being sought, such as citation rates and journal impact factors, which seem to be quantitative and objective indicators directly related to published science. The citation data are obtained from a database produced by the  Institute  for  Scientific  Information  (ISI)  in Philadelphia,  which  continuously  records  scientific citations as represented by the reference lists of articles from a large number of the world's scientific journals.The references are rearranged in the database to show how many times each publication has been cited within a certain period, and by whom, and the results are published as the Science Citation Index (SCI). On the basis  of  the Science  Citation  Index and  authors' publication lists, the annual citation rate of papers by a scientific  author  or  research  group  can  thus  be calculated. Similarly, the citation rate of a scientific journal—known as the journal impact factor—can be calculated as the mean citation rate of all the articles contained in the journal.4Journal impact factors, which are published annually in SCI Journal Citation Reports,are widely regarded as a quality ranking for journals and used extensively by leading journals in their advertising.Since  journal  impact  factors  are  so  readily available, it has been tempting to use them for evaluating individual scientists or research groups. On the assumption that the journal is representative of its articles, the journal impact factors of an author's articles can simply be added up to obtain an apparently objective and quantitative measure of the author's scientific achievement. In Italy, the use of journal impact factors was  recently  advocated  to  remedy  the  purported subjectivity  and  bias  in  appointments  to  higher academic positions.5In the Nordic countries, journal impact factors have, on occasion, been used in the evaluation of individuals as well as of institutions and have been proposed, or actually used, as one of the premises for allocation of university resources and positions.167Resource allocation based on impact factors  has  also  been  reported  from  Canada and Hungary and, colloquially, from several other countries. The  increasing  awareness  of  journal  impact factors, and the possibility of their use in evaluation, is already  changing  scientists'  publication  behaviour towards  publishing  in  journals  with  maximum impact,910often at the expense of specialist journals that might actually be more appropriate vehicles for the research in question.Given  the  increasing  use  of  journal  impact factors—as well as the (less explicit) use of journal prestige—in research evaluation, a critical examination of this indicator seems necessary (see box). 
